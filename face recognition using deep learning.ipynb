{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "entertaining-bracket",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import os \n",
    "from mtcnn import MTCNN\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "from matplotlib.patches import Rectangle\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from keras import Sequential\n",
    "from keras.regularizers import *\n",
    "from keras.applications import vgg19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "moderate-genesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIRECTORY = \"D:\\\\6th Sem\\\\AI\\\\ai\"\n",
    "detector = MTCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "headed-found",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_faces(img):\n",
    "    face = detector.detect_faces(img)\n",
    "    if len(face) == 0:\n",
    "        return face\n",
    "    x, y, width, height = face[0]['box']\n",
    "    return img[y:y+height+50 , x:x+width+50, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sixth-release",
   "metadata": {},
   "outputs": [],
   "source": [
    "users = {'Aman' : 0, 'Arjun' : 1, 'Divij' : 2, 'Soumya' : 3}\n",
    "images = []\n",
    "label = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fewer-missouri",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(TRAIN_DIRECTORY):\n",
    "    path = os.path.join(TRAIN_DIRECTORY,file)\n",
    "    for img1 in os.listdir(path):\n",
    "        img_path = os.path.join(path, img1)\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = detect_faces(img)\n",
    "        if(img.shape[0] == 0 or img.shape[1] == 0 or len(img) == 0):\n",
    "            continue\n",
    "        img = cv2.resize(img,(224,224))/255\n",
    "        images.append(img)\n",
    "        label.append(users[file])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "hungry-location",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['image'] = images\n",
    "df['label'] = label\n",
    "X_train=np.array(df.iloc[:,0].tolist())\n",
    "y_train = np.array(df.iloc[:,1].tolist())\n",
    "y_train = to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "universal-praise",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels.h5\n",
      "574717952/574710816 [==============================] - 181s 0us/step\n",
      "574726144/574710816 [==============================] - 181s 0us/step\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_1 (Batch (None, 224, 224, 3)       12        \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 4)                 16388     \n",
      "=================================================================\n",
      "Total params: 139,586,640\n",
      "Trainable params: 139,586,634\n",
      "Non-trainable params: 6\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "source_model = vgg19.VGG19(weights = 'imagenet')\n",
    "drop_layer = Dropout(0.5)\n",
    "drop_layer2 = Dropout(0.5)\n",
    "model = Sequential()\n",
    "for layer in source_model.layers[:-1]:\n",
    "    if layer == source_model.layers[-25]:\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(layer)\n",
    "model.add(Dense(4, activation=\"softmax\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "pressed-daily",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop\n",
    "lr = 1e-5\n",
    "decay = 1e-7 #0.0\n",
    "optimizer = RMSprop(lr=lr, decay=decay)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "excessive-coordination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "20/20 [==============================] - 59s 3s/step - loss: 3.5411 - accuracy: 0.2822\n",
      "Epoch 2/6\n",
      "20/20 [==============================] - 48s 2s/step - loss: 0.5434 - accuracy: 0.7082\n",
      "Epoch 3/6\n",
      "20/20 [==============================] - 48s 2s/step - loss: 0.0371 - accuracy: 1.0000\n",
      "Epoch 4/6\n",
      " 6/20 [========>.....................] - ETA: 37s - loss: 0.0133 - accuracy: 1.0000WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 120 batches). You may need to use the repeat() function when building your dataset.\n",
      "20/20 [==============================] - 16s 745ms/step - loss: 0.0130 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,y_train,steps_per_epoch = 20,shuffle = True,epochs = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "supreme-forestry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soumya\n",
      "[[0.04760016 0.12840787 0.04583082 0.7781611 ]]\n"
     ]
    }
   ],
   "source": [
    "user2 = {0:\"Aman\",1:\"Arjun\",2:\"Divij\",3:\"Soumya\"}\n",
    "temp = []\n",
    "for file in os.listdir(\"D:\\\\6th Sem\\\\AI\\\\test\"):\n",
    "    img_path = os.path.join(r'D:\\6th Sem\\AI\\test', file)\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = detect_faces(img)\n",
    "    if(img.shape[0] == 0 or img.shape[1] == 0 or len(img) == 0):\n",
    "        continue\n",
    "    img = cv2.resize(img,(224,224))/255.0\n",
    "    temp.append(img)\n",
    "    val = model.predict(np.array(temp))\n",
    "    cla = np.argmax(val, axis=1)\n",
    "    print(user2[cla[0]])\n",
    "    print(val)\n",
    "    temp = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-heater",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
